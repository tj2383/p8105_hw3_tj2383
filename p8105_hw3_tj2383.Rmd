---
title: "P8105_hw3_tj2383"
author: "Tanvi Jain"
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

##Problem 1

In this code chunk, I did some data cleaning:
-formatted the data to use appropriate variable names

-focused on the “Overall Health” topic

-included only responses from “Excellent” to “Poor”

-organized responses as a factor taking levels ordered from “Excellent” to “Poor”
```{r}
library(p8105.datasets)
data("brfss_smart2010")

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = factor(response, levels = ordered(c("Excellent", "Very good", "Good", "Fair", "Poor"))))
```

In the following code chunks I answer questions about the cleaned brfss_data:

In this code chunk I answer which states have 7 locations in 2002.
```{r}
brfss_data %>% 
  select(year, locationabbr, locationdesc) %>% 
  filter(year == "2002") %>% 
  distinct(locationdesc, locationabbr) %>% 
  group_by(locationabbr) %>% 
  summarize(locations_7 = n()) %>% 
  filter(locations_7 == "7")
```
In 2002, the states observed with 7 locations are CT, FL, and NC.

This is a spaghetti plot that shows the number of locations in each state from 2002-2010:
```{r}
brfss_data %>% 
  select(year, locationabbr, locationdesc) %>% 
  filter(year > 2001 & year < 2011) %>% 
  group_by(locationabbr, year) %>% 
  distinct(locationdesc, locationabbr) %>% 
  summarize(locations_state = n()) %>% 
  ggplot(aes(x = year, y = locations_state, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Locations in each state 2002-2010 plot",
    x = "year",
    y = "number of locations",
    caption = "Data from BRFSS_2010"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

In this code chunk I make a table of the mean and standard deviation of proportion of "Excellent" responses in NY State in 2002, 2006, and 2010.
```{r}
brfss_data %>%
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>% 
  select(year, excellent, locationabbr) %>% 
  filter(!is.na(excellent) & locationabbr == "NY" & (year == "2002" | year == "2006" | year == "2010")) %>%
  group_by(year) %>% 
  summarize(mean_proportion_excellent = mean(excellent), sd_proportion_excellent = sd(excellent)) %>% 
  knitr::kable()
```

In this code chunk I compute the average proportion in each response category (taking the average across locations in a state) for each year and state. Then I make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r}
brfss_data %>% 
  select(year, locationabbr, response, data_value) %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(mean_response = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_response, color = locationabbr)) + 
  geom_point() +
  facet_grid(~response) +
  labs(
    title = "Average proportion of response values in each state for each year plot",
    x = "year",
    y = "average proportion of response",
    caption = "Data from BRFSS_2010"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) + 
  theme_bw() + 
  theme(legend.position = "bottom")
```

##Problem 2

Testing inline code:
```{r}
library(p8105.datasets)
data("instacart")

instacart_data = instacart %>% 
  janitor::clean_names()

nrow(instacart_data)
ncol(instacart_data)

instacart_data %>% 
  count()

instacart_data %>% 
  distinct(user_id) %>% 
  count()

instacart_data %>% 
  select(user_id) %>% 
  head(1)

instacart_data %>% 
  select(product_name) %>% 
  head(1)

instacart_data %>% 
  select(department) %>% 
  head(1)
```
The size of the instacart dataset is `r nrow(instacart_data)` rows and `r ncol(instacart_data)` columns. It is an anonymized dataset with grocery orders from instacart users, however, it is not generalizable to the broader instacart user population because it is not a random sample of products, users, or purchases. The dataset contains `r count(instacart_data)` observations and `r instacart_data %>% distinct(user_id) %>% count()` distinct users. Some key variables in the dataset include `user_id` such as `r instacart_data %>% select(user_id) %>% head(1)` and the `product_name` variable indicates a product the user purchased, in thiscase `r instacart_data %>% select(product_name) %>% head(1)` and the `department` variable indicates which deparment this product came from, in this case, `r instacart_data %>% select(department) %>% head(1)`.


In the following code chunk I answer questions about instacart_data:
```{r}
instacart_data %>% 
  select(aisle) %>% 
  count()

instacart_data %>% 
  group_by(aisle) %>% 
  summarize(amount_ordered = n()) %>% 
  arrange(desc(amount_ordered)) %>% 
  select(aisle) %>% 
  head(1)
```
There are `r instacart_data %>% select(aisle) %>% count()` aisles and the most items are ordered from the following aisle: `r instacart_data %>% group_by(aisle) %>% summarize(amount_ordered = n()) %>% arrange(desc(amount_ordered)) %>% select(aisle) %>% head(1)`.

The following plot shows the number of items ordered in each distinct aisle:
```{r}
instacart_data %>% 
  group_by(aisle) %>% 
  summarize(amount_ordered = n()) %>% 
  ggplot(aes(x = aisle, y = amount_ordered)) + 
  geom_point() +
  labs(
     title = "Items ordered in each aisle",
     x = "aisle",
     y = "number of items ordered"
    ) +
    theme_bw() +
    theme(legend.position = "bottom")
```

In the following code chunk I create a table of the most popular item in the aisles `baking ingredients`, `dog food care`, and `packaged vegetables and fruits`.
```{r}
instacart_data %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(number_ordered = n()) %>% 
  arrange(desc(number_ordered)) %>% 
  group_by(aisle) %>% 
  top_n(n = 1) %>% 
  rename(most_popular_item = product_name) %>% 
  knitr::kable()
```

In the following code chunk I create a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```

##Problem 3

Testing in line code for a description of the dataset:
```{r}
library(p8105.datasets)
data(ny_noaa)

nrow(ny_noaa)
ncol(ny_noaa)

ny_noaa %>% 
  count()

ny_noaa %>% 
  distinct(id) %>% 
  count()

ny_noaa %>% 
  select(date) %>% 
  head(1)

ny_noaa %>% 
  select(prcp) %>% 
  head(1)

ny_noaa %>% 
  select(tmax) %>% 
  head(1)

ny_noaa %>% 
  select(tmin) %>% 
  head(1)

ny_noaa %>% 
  filter(is.na(tmax) | is.na(tmin) | is.na(prcp) | is.na(snow) | is.na(snwd)) %>% 
  nrow()
```
The size of the NOAA dataset is `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. This public dataset provides information on weather and this specific version contains information from weather stations in NY state. The dataset contains `r count(ny_noaa)` observations and `r ny_noaa %>% distinct(id) %>% count()` distinct weather stations. Some key variables in the dataset include `id` such as `r ny_noaa %>% select(id) %>% head(1)` and the `date` variable indicates the date of the weather observation `r ny_noaa %>% select(date) %>% head(1)`. The `prcp` variable indicates the precipitation that occurred on this date. The `tmax` and `tmin` variable tell us the maximum and minimum temperatures on this date. There are several missing data values in this data set, especially for prcp, tmin, and tmax, precisely `r ny_noaa %>% filter(is.na(tmax) | is.na(tmin) | is.na(prcp) | is.na(snow) | is.na(snwd)) %>% nrow()` missing observations.


In this code chunk I clean the dataset ny_noaa_data:
-created separate variables for year, month, and day
-ensure observations for temperature, precipitation, and snowfall are given in reasonable units
```{r}
ny_noaa_data = ny_noaa %>%
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(prcp = prcp / 10, tmax = as.integer(tmax) / 10, tmin = as.integer(tmin) / 10)

ny_noaa_data %>% 
  group_by(snow) %>% 
  summarize(freq_observed = n()) %>% 
  arrange(desc(freq_observed)) %>% 
  head(1)
```
The most commonly observed value for snowfall was 0 mm because snowfall is not observed at high frequency throughout the year especially during the spring, summer, and fall months.


The following plot: Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
ny_noaa_data %>% 
  filter(tmax != "NA", month == "01" | month == "07") %>%
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_point() +
  facet_grid(~month) +
  labs(
      x = "year",
      y = "average temperature (°C)",
      title = "Average temperatures for january and july at New York weather stations",
      caption = "Data from NOAA"
    ) +
    viridis::scale_color_viridis(
      discrete = TRUE,
      name = "ID"
    ) +
    theme_bw() +
    theme(legend.position = "bottom")
```







